{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "# Introduction\n",
    "Examining shoeprint evidence found at a crime scene assists investigators in identifying suspects of the crime. We introduce a tool to retrieve the closest matching shoe models to query crime-scene prints. The tool uses CriSp in its backend to match the crime-scene print to a large-scale database of tread depth maps. The details of this method can be found in its [project page](https://github.com/Samia067/Crisp) and paper: [CriSp: Leveraging Tread Depth Maps for Enhanced Crime-Scene Shoeprint Matching](https://arxiv.org/abs/2404.16972), currently under review at [ECCV 2024](https://eccv.ecva.net/).\n",
    "\n",
    "**How the retrieval works**:\n",
    "We precompute and store features for the depth maps in the reference database.\n",
    "When a query crime-scene sheoprint and a corresponding mask is uploaded to the system, we compute its features using CriSp. We mask out irrelevant portions of both the query and database features with the provided mask. Similarity between query and database images are computed using cosine similarity and a ranked list is generated. \n",
    "\n",
    "**Reference Database statistics**:\n",
    "The reference database used contains depth maps from 56,847 shoe tread instances and 24,766 shoe models. \n",
    "\n",
    "# How to use this tool\n",
    "steps to make a query\n",
    "1. Run the code. On the first run, wait for upto 5 minutes while the model and dataset are downloaded. \n",
    "2. After the download is complete, you will see a box where you can specify the number of results you want to see for each query, a file uploader for the query print and another for the mask. If  you do not have a preference for the number of results to view, you can leave it at the default value of 10. \n",
    "3. Select a query print using the button labeled 'Upload Print'. The print image should be of size 384x192. If a different sized image is uploaded, it will be resized to this shape. The query print should be modified such that the print lies on the correct position on the yellow shoe outline visualized. \n",
    "4. Next, upload a mask for the print where only the visible portions have a value of 1 and the rest is 0.\n",
    "5. Once you select the query print and mask, you can hit 'Search' and you will see a ranked list of results. \n",
    "\n",
    "Example query prints\n",
    "\n",
    "# Contact\n",
    "\n",
    "Please feel free to email me at (sshafiqu [at] uci [dot] edu) if you have any questions. Author list:\n",
    "\n",
    "<table>\n",
    "<tr>\n",
    "<td> <img src=\"git/figures/samia.jpg\" alt=\"Drawing\" style=\"width: 145px;\"/> </td>\n",
    "<td>\n",
    "    <p>Samia Shafique</p>\n",
    "    <p>PhD Student</p>\n",
    "    <p>University of California, Irvine</p>\n",
    "</td>\n",
    "\n",
    "</tr>\n",
    "<tr>\n",
    "<td> <img src=\"git/figures/charless.jpg\" alt=\"Drawing\" style=\"width: 145px;\"/> </td>\n",
    "<td>\n",
    "    <p>Charless Fowlkes</p>\n",
    "    <p>Professor</p>\n",
    "    <p>University of California, Irvine</p>\n",
    "</td>\n",
    "</tr>\n",
    "</table>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63739a1d0f5d48418dfdc0ac71e45f50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntText(value=10, description='Count'), FileUpload(value={}, description='Upload Print'), FileU…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#@title ##← Click on the circled arrow and wait for up to 5 minutes\n",
    "\n",
    "import sys\n",
    "# !{sys.executable} -m pip install torch==1.11.0+cu102  --extra-index-url https://download.pytorch.org/whl/cu102\n",
    "\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import requests\n",
    "import numpy as np\n",
    "import os\n",
    "import urllib.request\n",
    "import numpy as np\n",
    "\n",
    "from IPython.display import display, Markdown, HTML, clear_output\n",
    "import ipywidgets as widgets\n",
    "import io\n",
    "from PIL import Image\n",
    "import zipfile\n",
    "import csv\n",
    "\n",
    "class Bottleneck(nn.Module):\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1, is_last=False):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        self.is_last = is_last\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.conv3 = nn.Conv2d(planes, self.expansion * planes, kernel_size=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(self.expansion * planes)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != self.expansion * planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.expansion * planes, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(self.expansion * planes)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = F.relu(self.bn2(self.conv2(out)))\n",
    "        out = self.bn3(self.conv3(out))\n",
    "        out += self.shortcut(x)\n",
    "        preact = out\n",
    "        out = F.relu(out)\n",
    "        if self.is_last:\n",
    "            return out, preact\n",
    "        else:\n",
    "            return out\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, num_blocks, in_channel=2, zero_init_residual=False, final_pool=False):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_planes = 64\n",
    "        self.final_pool = final_pool\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channel, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=2)\n",
    "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
    "        if self.final_pool:\n",
    "            self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "        # Zero-initialize the last BN in each residual branch,\n",
    "        # so that the residual branch starts with zeros, and each residual block behaves\n",
    "        # like an identity. This improves the model by 0.2~0.3% according to:\n",
    "        # https://arxiv.org/abs/1706.02677\n",
    "        if zero_init_residual:\n",
    "            for m in self.modules():\n",
    "                nn.init.constant_(m.bn3.weight, 0)\n",
    "\n",
    "    def _make_layer(self, block, planes, num_blocks, stride, maxpool=False, maxpool2=False):\n",
    "        strides = [stride] + [1] * (num_blocks - 1)\n",
    "        layers = []\n",
    "        for i in range(num_blocks):\n",
    "            stride = strides[i]\n",
    "            layers.append(block(self.in_planes, planes, stride))\n",
    "            self.in_planes = planes * block.expansion\n",
    "            if maxpool2:\n",
    "                layers.append(nn.MaxPool2d(kernel_size=3, stride=2, padding=1))\n",
    "        if maxpool:\n",
    "            layers.append(nn.MaxPool2d(kernel_size=3, stride=2, padding=1))\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out1 = self.layer4(out)\n",
    "        if self.final_pool:\n",
    "            if mask is not None:\n",
    "                # mask penultimate features out1\n",
    "                # in other words, average over only the relavant portions of the image\n",
    "                out1[~mask.expand_as(out1)] = 0\n",
    "\n",
    "            out = self.avgpool(out1)\n",
    "            out = torch.flatten(out, 1)\n",
    "            return out\n",
    "        return out1\n",
    "\n",
    "    \n",
    "class MatchingModel(nn.Module):\n",
    "    \"\"\"backbone + projection head\"\"\"\n",
    "    # this is different from SupConResNet because\n",
    "    def __init__(self, feat_dim=128, in_channel=2, feature_dim=(1,1)):\n",
    "        super(MatchingModel, self).__init__()\n",
    "        # model_fun, dim_in = model_dict[name]\n",
    "        # self.encoder = model_fun(in_channel=in_channel, final_pool=False)\n",
    "        self.encoder = ResNet(Bottleneck, [3, 4, 6, 3], in_channel=in_channel, final_pool=False)\n",
    "        dim_in = 2048\n",
    "        self.head = nn.Sequential(\n",
    "                nn.Conv2d(dim_in, dim_in, 1),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Conv2d(dim_in, feat_dim, 1)\n",
    "            )\n",
    "        self.feature_dim=feature_dim\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d(self.feature_dim)\n",
    "\n",
    "    def forward(self, x, spatial_feat=False, mask=None):\n",
    "        feat = self.encoder(x, mask=mask)\n",
    "        embedded = self.head(feat)\n",
    "        if mask is not None:\n",
    "            embedded[~mask.expand_as(embedded)] = 0\n",
    "\n",
    "        return embedded if spatial_feat else self.vectorize(embedded) \n",
    "\n",
    "    def vectorize(self, x, spatial=False):\n",
    "        if not spatial or self.feature_dim!=(1,1):\n",
    "            x = self.avgpool(x)\n",
    "        return torch.flatten(F.normalize(x, dim=1), 1)\n",
    "\n",
    "    \n",
    "\n",
    "def download(id, destination, download_name=None):\n",
    "    download_name = download_name if download_name else destination\n",
    "    if not os.path.exists(download_name):\n",
    "        URL = \"https://docs.google.com/uc?export=download\"\n",
    "\n",
    "        session = requests.Session()\n",
    "\n",
    "        response = session.get(URL, params = { 'id' : id }, stream = True)\n",
    "        token = get_confirm_token(response)\n",
    "        \n",
    "        if token:\n",
    "            # params = { 'id' : id, 'confirm' : token }\n",
    "            URL= 'https://drive.usercontent.google.com/download?export=download&confirm=t'\n",
    "            params = { 'id' : id}\n",
    "            response = session.get(URL, params = params, stream = True)\n",
    "\n",
    "        save_response_content(response, destination)  \n",
    "\n",
    "def get_confirm_token(response):\n",
    "    if 'too large for Google to scan for viruses. Would you still like to download this file?' in response.text:\n",
    "        return True\n",
    "\n",
    "def save_response_content(response, destination):\n",
    "    CHUNK_SIZE = 32768\n",
    "\n",
    "    with open(destination, \"wb\") as f:\n",
    "        for chunk in response.iter_content(CHUNK_SIZE):\n",
    "            if chunk: # filter out keep-alive new chunks\n",
    "                f.write(chunk)\n",
    "\n",
    "\n",
    "def load_model(weights, file_id):\n",
    "    net = MatchingModel().to(device)\n",
    "    download(file_id, weights)\n",
    "    file = torch.load(weights, map_location=device)\n",
    "    net.load_state_dict(file)\n",
    "    return net\n",
    "\n",
    "\n",
    "def display_results(query_image, match_names, match_values):\n",
    "    clear_output()\n",
    "    display(search_widget)\n",
    "    display(query_image)\n",
    "    for i, match_name in enumerate(match_names):\n",
    "        \n",
    "        wi_rank = widgets.HTML(value=str(i+1) + \"          \")\n",
    "\n",
    "        path = os.path.join(database_name, 'image', match_name)\n",
    "        img1 = open(path, 'rb').read()\n",
    "        wi1 = widgets.Image(value=img1)\n",
    "        \n",
    "        path = os.path.join(database_name, 'print', match_name)\n",
    "        img2 = open(path, 'rb').read()\n",
    "        wi2 = widgets.Image(value=img2)\n",
    "        \n",
    "        # metadata information\n",
    "        shoeid = int(match_name[:6])\n",
    "        metadata_textboxes = []\n",
    "        for header in ['brand', 'product', 'gender']:\n",
    "            if shoeid in metadata:\n",
    "                metadata_textboxes.append(widgets.Text(\n",
    "                    value=header + ': ' + metadata[shoeid][header],\n",
    "                    disabled=True   \n",
    "                ))\n",
    "        \n",
    "        button = widgets.Button(description=\"More like this\")\n",
    "        button.filename = match_name\n",
    "        button.on_click(on_show_more_button_clicked)\n",
    "        \n",
    "        box_layout = widgets.Layout(display='flex',\n",
    "            flex_flow='row',\n",
    "            align_items='center')\n",
    "        metadata_textboxes.append(button)\n",
    "        \n",
    "        metadata_widget = widgets.VBox(metadata_textboxes, layout=widgets.Layout(justify_content='center'))\n",
    "    \n",
    "        display(widgets.HBox([wi_rank, wi1, wi2, metadata_widget], layout=box_layout))\n",
    "    \n",
    "\n",
    "def outline_image(image, outline):\n",
    "    image = np.array(image)\n",
    "    if len(image.shape) == 2:\n",
    "        image = image[:, :, np.newaxis].repeat(3, axis=2)    \n",
    "    if np.max(image) == 1:\n",
    "        image = image *255\n",
    "    outline_mask = np.array(outline)[:,:, 3] != 0\n",
    "    outline_color = np.array(outline)[0,0,0:3]\n",
    "#     image[outline_mask] = (image[outline_mask] + outline_color)/2\n",
    "    image[outline_mask] = outline_color\n",
    "    return Image.fromarray(image)\n",
    "    \n",
    "\n",
    "def on_button_clicked(b):\n",
    "    # print(print_uploader.value.items())\n",
    "    # print(mask_uploader.value.items())\n",
    "    for (print_name, print_file_info), (mask_name, mask_file_info) in zip(print_uploader.value.items(), mask_uploader.value.items()):\n",
    "        img = Image.open(io.BytesIO(print_file_info['content']))\n",
    "        query_image = img.resize((384, 192))\n",
    "        mask = Image.open(io.BytesIO(mask_file_info['content']))\n",
    "        mask = mask.resize((384, 192))\n",
    "        n_results = result_count.value\n",
    "        match_names, match_values = image_search(query_image, mask, n_results)\n",
    "        print(match_values)\n",
    "        outlined_query = outline_image(query_image, outline)\n",
    "        display_results(outlined_query, match_names, match_values)\n",
    "        \n",
    "\n",
    "def on_show_more_button_clicked(b):\n",
    "    path = os.path.join(database_name, 'print', b.filename)\n",
    "    query = Image.open(path)\n",
    "    n_results = result_count.value\n",
    "    match_names, match_values = image_search(query, n_results)\n",
    "    outlined_query = outline_image(query, outline)\n",
    "    display_results(outlined_query, match_names, match_values)\n",
    "    \n",
    "def image_search(query_image, mask, n_results=24):\n",
    "    img = np.array(query_image)\n",
    "    if len(img.shape) == 3:\n",
    "        img = np.mean(img, axis=2)\n",
    "    img = torch.tensor(img/255.0).to(device).unsqueeze(0).unsqueeze(0)\n",
    "    zeros = torch.zeros(img.shape).to(device)\n",
    "    query = torch.cat((img, zeros), dim=1).float()\n",
    "\n",
    "    mask = np.array(mask)\n",
    "    if len(mask.shape) == 3:\n",
    "        mask = np.mean(mask, axis=2) \n",
    "    mask = torch.tensor(mask/255.0).to(device).unsqueeze(0).unsqueeze(0) > 0.5\n",
    "    \n",
    "    net.eval()\n",
    "    with torch.no_grad():\n",
    "        embedded_mask = (F.interpolate(mask.float(), (6, 12), mode='bilinear') > 0)\n",
    "        query_features = net(query, spatial_feat=True)\n",
    "        query_features = net.vectorize((query_features*embedded_mask.to(query_features.device)).to(query.device), spatial=True)\n",
    "        query_dot_dataset = torch.matmul(query_features, database_features.T)\n",
    "        values, indices = torch.sort(query_dot_dataset, dim=1, descending=True)\n",
    "        indices = indices.squeeze().detach().cpu().numpy()\n",
    "        match_names = database_names[indices][:n_results]\n",
    "        values = values.squeeze().detach().cpu().numpy()\n",
    "        match_values = values[:n_results]\n",
    "    return match_names, match_values\n",
    "\n",
    "device = torch.device('cuda:0') if torch.cuda.is_available() else torch.device('cpu')  \n",
    "print(device)\n",
    "    \n",
    "print('==> Loading model')\n",
    "file_id = '1X8b75CWiFl11VSkhOG0XelQlZrXGhC_Y'\n",
    "# https://drive.google.com/file/d/1FFSh0uSUr5c4ebaCkzIELv98wRizSvrM/view?usp=sharing\n",
    "file_id = '1FFSh0uSUr5c4ebaCkzIELv98wRizSvrM'\n",
    "net = load_model('model.pth', file_id)\n",
    "\n",
    "print('==> Loading database')\n",
    "database_name = 'database'\n",
    "file_id = '1v4DYJ1hF4UadsMOa3yachsORPxwpHQpz'\n",
    "download(file_id, database_name + '.zip', download_name='database.zip')\n",
    "if not os.path.exists(database_name):\n",
    "    with zipfile.ZipFile(database_name + '.zip', 'r') as zip_ref:\n",
    "        zip_ref.extractall()\n",
    "database_feat_name = os.path.join(database_name, 'features.pth')\n",
    "database_features = torch.load(database_feat_name, map_location=device).to(device)\n",
    "database_names = os.path.join(database_name, 'names.npy')\n",
    "database_names = np.load(database_names)\n",
    "\n",
    "# download outline\n",
    "outline_name = 'outline.png'\n",
    "file_id = '1_34Waq_gd7o2KzOjVgUMtVS_3B-KNmSb'\n",
    "download(file_id, outline_name)\n",
    "outline = Image.open(outline_name)\n",
    "\n",
    "clear_output()\n",
    "search_button = widgets.Button(description=\"Search\")\n",
    "# output = widgets.Output()\n",
    "print_uploader = widgets.FileUpload(multiple=False, description='Upload Print')\n",
    "mask_uploader = widgets.FileUpload(multiple=False, description='Upload Mask')\n",
    "result_count = widgets.IntText(\n",
    "    value=10,\n",
    "    description='Count',\n",
    "    disabled=False\n",
    ")\n",
    "search_widget = widgets.HBox([result_count, print_uploader, mask_uploader, search_button], layout=widgets.Layout(justify_content='center'))\n",
    "display(search_widget)\n",
    "search_button.on_click(on_button_clicked)\n",
    "\n",
    "# get metadata\n",
    "with open(os.path.join(database_name, 'metadata.csv'), newline='') as csvfile:\n",
    "    reader = csv.DictReader(csvfile)\n",
    "    metadata = {}\n",
    "    metadata_headers = ['shoeid', 'gender', 'brand', 'product', 'category', 'prints']\n",
    "    for row in reader:\n",
    "        shoeid, gender, brand, product, category, prints = [row[header] for header in metadata_headers]\n",
    "        metadata[int(shoeid)] = {metadata_headers[i+1]: md for i, md in enumerate([gender, brand, product, category, prints])}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
