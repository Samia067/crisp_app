{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "# Introduction\n",
    "Examining shoeprint evidence found at a crime scene assists investigators in identifying suspects of the crime. We introduce a tool to retrieve the closest matching shoe models to query crime-scene prints. The tool uses CriSp in its backend to match the crime-scene print to a large-scale database of tread depth maps. The details of this method can be found in its [project page](https://github.com/Samia067/Crisp) and paper: [CriSp: Leveraging Tread Depth Maps for Enhanced Crime-Scene Shoeprint Matching](https://arxiv.org/abs/2404.16972), currently under review at [ECCV 2024](https://eccv.ecva.net/).\n",
    "\n",
    "**How the retrieval works**:\n",
    "We precompute and store features for the depth maps in the reference database.\n",
    "When a query crime-scene sheoprint and a corresponding mask is uploaded to the system, we compute its features using CriSp. We mask out irrelevant portions of both the query and database features with the provided mask. Similarity between query and database images are computed using cosine similarity and a ranked list is generated. \n",
    "\n",
    "**Reference Database statistics**:\n",
    "The reference database used contains depth maps from 56,847 shoe tread instances and 24,766 shoe models. \n",
    "\n",
    "# How to use this tool\n",
    "steps to make a query\n",
    "1. Run the code. On the first run, wait for upto 5 minutes while the model and dataset are downloaded. \n",
    "2. After the download is complete, you will see a box where you can specify the number of results you want to see for each query, a file uploader for the query print and another for the mask. If  you do not have a preference for the number of results to view, you can leave it at the default value of 10. \n",
    "3. Select a query print using the button labeled 'Upload Print'. The print image should be of size 384x192. If a different sized image is uploaded, it will be resized to this shape. The query print should be modified such that the print lies on the correct position on the yellow shoe outline visualized. \n",
    "4. Next, upload a mask for the print where only the visible portions have a value of 1 and the rest is 0.\n",
    "5. Once you select the query print and mask, you can hit 'Search' and you will see a ranked list of results. \n",
    "\n",
    "Example query prints\n",
    "\n",
    "# Contact\n",
    "\n",
    "Please feel free to email me at (sshafiqu [at] ics [dot] uci [dot] edu) if you have any questions. Here is a list of the authors of this tool:\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src='git/figures/architecture.png' width='500'/>\n",
    "</p>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n",
      "==> Loading model\n",
      "==> Loading database\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 1.95 GiB (GPU 0; 10.76 GiB total capacity; 2.06 GiB already allocated; 985.62 MiB free; 2.16 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-ed4b849c2fff>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    307\u001b[0m         \u001b[0mzip_ref\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextractall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    308\u001b[0m \u001b[0mdatabase_feat_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatabase_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'features.pth'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 309\u001b[0;31m \u001b[0mdatabase_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatabase_feat_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    310\u001b[0m \u001b[0mdatabase_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatabase_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'names.npy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    311\u001b[0m \u001b[0mdatabase_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatabase_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    710\u001b[0m                     \u001b[0mopened_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseek\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morig_position\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    711\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 712\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0m_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_zipfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    713\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_legacy_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_load\u001b[0;34m(zip_file, map_location, pickle_module, pickle_file, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1044\u001b[0m     \u001b[0munpickler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mUnpicklerWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1045\u001b[0m     \u001b[0munpickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpersistent_load\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpersistent_load\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1046\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munpickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1047\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1048\u001b[0m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_loaded_sparse_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36mpersistent_load\u001b[0;34m(saved_id)\u001b[0m\n\u001b[1;32m   1014\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mloaded_storages\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1015\u001b[0m             \u001b[0mnbytes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumel\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_element_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1016\u001b[0;31m             \u001b[0mload_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_maybe_decode_ascii\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1017\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1018\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloaded_storages\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload_tensor\u001b[0;34m(dtype, numel, key, location)\u001b[0m\n\u001b[1;32m    999\u001b[0m         \u001b[0;31m# stop wrapping with _TypedStorage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1000\u001b[0m         loaded_storages[key] = torch.storage._TypedStorage(\n\u001b[0;32m-> 1001\u001b[0;31m             \u001b[0mwrap_storage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrestore_location\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1002\u001b[0m             dtype=dtype)\n\u001b[1;32m   1003\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36mrestore_location\u001b[0;34m(storage, location)\u001b[0m\n\u001b[1;32m    971\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap_location\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    972\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mrestore_location\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 973\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mdefault_restore_location\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap_location\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    974\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    975\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mrestore_location\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36mdefault_restore_location\u001b[0;34m(storage, location)\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mdefault_restore_location\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_package_registry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 176\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    177\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_cuda_deserialize\u001b[0;34m(obj, location)\u001b[0m\n\u001b[1;32m    156\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mstorage_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnbytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 158\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/_utils.py\u001b[0m in \u001b[0;36m_cuda\u001b[0;34m(self, device, non_blocking, **kwargs)\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m             \u001b[0mnew_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mnew_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36m_lazy_new\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    659\u001b[0m     \u001b[0;31m# We may need to call lazy init again if we are a forked child\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    660\u001b[0m     \u001b[0;31m# del _CudaBase.__new__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 661\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_CudaBase\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__new__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    662\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    663\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 1.95 GiB (GPU 0; 10.76 GiB total capacity; 2.06 GiB already allocated; 985.62 MiB free; 2.16 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "#@title ##â† Click on the circled arrow and wait for up to 5 minutes\n",
    "\n",
    "import sys\n",
    "# !{sys.executable} -m pip install torch==1.11.0+cu102  --extra-index-url https://download.pytorch.org/whl/cu102\n",
    "\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import requests\n",
    "import numpy as np\n",
    "import os\n",
    "import urllib.request\n",
    "import numpy as np\n",
    "\n",
    "from IPython.display import display, Markdown, HTML, clear_output\n",
    "import ipywidgets as widgets\n",
    "import io\n",
    "from PIL import Image\n",
    "import zipfile\n",
    "import csv\n",
    "\n",
    "class Bottleneck(nn.Module):\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1, is_last=False):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        self.is_last = is_last\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.conv3 = nn.Conv2d(planes, self.expansion * planes, kernel_size=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(self.expansion * planes)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != self.expansion * planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.expansion * planes, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(self.expansion * planes)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = F.relu(self.bn2(self.conv2(out)))\n",
    "        out = self.bn3(self.conv3(out))\n",
    "        out += self.shortcut(x)\n",
    "        preact = out\n",
    "        out = F.relu(out)\n",
    "        if self.is_last:\n",
    "            return out, preact\n",
    "        else:\n",
    "            return out\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, num_blocks, in_channel=2, zero_init_residual=False, final_pool=False):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_planes = 64\n",
    "        self.final_pool = final_pool\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channel, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=2)\n",
    "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
    "        if self.final_pool:\n",
    "            self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "        # Zero-initialize the last BN in each residual branch,\n",
    "        # so that the residual branch starts with zeros, and each residual block behaves\n",
    "        # like an identity. This improves the model by 0.2~0.3% according to:\n",
    "        # https://arxiv.org/abs/1706.02677\n",
    "        if zero_init_residual:\n",
    "            for m in self.modules():\n",
    "                nn.init.constant_(m.bn3.weight, 0)\n",
    "\n",
    "    def _make_layer(self, block, planes, num_blocks, stride, maxpool=False, maxpool2=False):\n",
    "        strides = [stride] + [1] * (num_blocks - 1)\n",
    "        layers = []\n",
    "        for i in range(num_blocks):\n",
    "            stride = strides[i]\n",
    "            layers.append(block(self.in_planes, planes, stride))\n",
    "            self.in_planes = planes * block.expansion\n",
    "            if maxpool2:\n",
    "                layers.append(nn.MaxPool2d(kernel_size=3, stride=2, padding=1))\n",
    "        if maxpool:\n",
    "            layers.append(nn.MaxPool2d(kernel_size=3, stride=2, padding=1))\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out1 = self.layer4(out)\n",
    "        if self.final_pool:\n",
    "            if mask is not None:\n",
    "                # mask penultimate features out1\n",
    "                # in other words, average over only the relavant portions of the image\n",
    "                out1[~mask.expand_as(out1)] = 0\n",
    "\n",
    "            out = self.avgpool(out1)\n",
    "            out = torch.flatten(out, 1)\n",
    "            return out\n",
    "        return out1\n",
    "\n",
    "    \n",
    "class MatchingModel(nn.Module):\n",
    "    \"\"\"backbone + projection head\"\"\"\n",
    "    # this is different from SupConResNet because\n",
    "    def __init__(self, feat_dim=128, in_channel=2, feature_dim=(1,1)):\n",
    "        super(MatchingModel, self).__init__()\n",
    "        # model_fun, dim_in = model_dict[name]\n",
    "        # self.encoder = model_fun(in_channel=in_channel, final_pool=False)\n",
    "        self.encoder = ResNet(Bottleneck, [3, 4, 6, 3], in_channel=in_channel, final_pool=False)\n",
    "        dim_in = 2048\n",
    "        self.head = nn.Sequential(\n",
    "                nn.Conv2d(dim_in, dim_in, 1),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Conv2d(dim_in, feat_dim, 1)\n",
    "            )\n",
    "        self.feature_dim=feature_dim\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d(self.feature_dim)\n",
    "\n",
    "    def forward(self, x, spatial_feat=False, mask=None):\n",
    "        feat = self.encoder(x, mask=mask)\n",
    "        embedded = self.head(feat)\n",
    "        if mask is not None:\n",
    "            embedded[~mask.expand_as(embedded)] = 0\n",
    "\n",
    "        return embedded if spatial_feat else self.vectorize(embedded) \n",
    "\n",
    "    def vectorize(self, x, spatial=False):\n",
    "        if not spatial or self.feature_dim!=(1,1):\n",
    "            x = self.avgpool(x)\n",
    "        return torch.flatten(F.normalize(x, dim=1), 1)\n",
    "\n",
    "    \n",
    "\n",
    "def download(id, destination, download_name=None):\n",
    "    download_name = download_name if download_name else destination\n",
    "    if not os.path.exists(download_name):\n",
    "        URL = \"https://docs.google.com/uc?export=download\"\n",
    "\n",
    "        session = requests.Session()\n",
    "\n",
    "        response = session.get(URL, params = { 'id' : id }, stream = True)\n",
    "        token = get_confirm_token(response)\n",
    "        \n",
    "        if token:\n",
    "            # params = { 'id' : id, 'confirm' : token }\n",
    "            URL= 'https://drive.usercontent.google.com/download?export=download&confirm=t'\n",
    "            params = { 'id' : id}\n",
    "            response = session.get(URL, params = params, stream = True)\n",
    "\n",
    "        save_response_content(response, destination)  \n",
    "\n",
    "def get_confirm_token(response):\n",
    "    if 'too large for Google to scan for viruses. Would you still like to download this file?' in response.text:\n",
    "        return True\n",
    "\n",
    "def save_response_content(response, destination):\n",
    "    CHUNK_SIZE = 32768\n",
    "\n",
    "    with open(destination, \"wb\") as f:\n",
    "        for chunk in response.iter_content(CHUNK_SIZE):\n",
    "            if chunk: # filter out keep-alive new chunks\n",
    "                f.write(chunk)\n",
    "\n",
    "\n",
    "def load_model(weights, file_id):\n",
    "    net = MatchingModel().to(device)\n",
    "    download(file_id, weights)\n",
    "    file = torch.load(weights, map_location=device)\n",
    "    net.load_state_dict(file)\n",
    "    return net\n",
    "\n",
    "\n",
    "def display_results(query_image, match_names, match_values):\n",
    "    clear_output()\n",
    "    display(search_widget)\n",
    "    display(query_image)\n",
    "    for i, match_name in enumerate(match_names):\n",
    "        \n",
    "        wi_rank = widgets.HTML(value=str(i+1) + \"          \")\n",
    "\n",
    "        path = os.path.join(database_name, 'image', match_name)\n",
    "        img1 = open(path, 'rb').read()\n",
    "        wi1 = widgets.Image(value=img1)\n",
    "        \n",
    "        path = os.path.join(database_name, 'print', match_name)\n",
    "        img2 = open(path, 'rb').read()\n",
    "        wi2 = widgets.Image(value=img2)\n",
    "        \n",
    "        # metadata information\n",
    "        shoeid = int(match_name[:6])\n",
    "        metadata_textboxes = []\n",
    "        for header in ['brand', 'product', 'gender']:\n",
    "            if shoeid in metadata:\n",
    "                metadata_textboxes.append(widgets.Text(\n",
    "                    value=header + ': ' + metadata[shoeid][header],\n",
    "                    disabled=True   \n",
    "                ))\n",
    "        \n",
    "        button = widgets.Button(description=\"More like this\")\n",
    "        button.filename = match_name\n",
    "        button.on_click(on_show_more_button_clicked)\n",
    "        \n",
    "        box_layout = widgets.Layout(display='flex',\n",
    "            flex_flow='row',\n",
    "            align_items='center')\n",
    "        metadata_textboxes.append(button)\n",
    "        \n",
    "        metadata_widget = widgets.VBox(metadata_textboxes, layout=widgets.Layout(justify_content='center'))\n",
    "    \n",
    "        display(widgets.HBox([wi_rank, wi1, wi2, metadata_widget], layout=box_layout))\n",
    "    \n",
    "\n",
    "def outline_image(image, outline):\n",
    "    image = np.array(image)\n",
    "    if len(image.shape) == 2:\n",
    "        image = image[:, :, np.newaxis].repeat(3, axis=2)    \n",
    "    if np.max(image) == 1:\n",
    "        image = image *255\n",
    "    outline_mask = np.array(outline)[:,:, 3] != 0\n",
    "    outline_color = np.array(outline)[0,0,0:3]\n",
    "#     image[outline_mask] = (image[outline_mask] + outline_color)/2\n",
    "    image[outline_mask] = outline_color\n",
    "    return Image.fromarray(image)\n",
    "    \n",
    "\n",
    "def on_button_clicked(b):\n",
    "    # print(print_uploader.value.items())\n",
    "    # print(mask_uploader.value.items())\n",
    "    for (print_name, print_file_info), (mask_name, mask_file_info) in zip(print_uploader.value.items(), mask_uploader.value.items()):\n",
    "        img = Image.open(io.BytesIO(print_file_info['content']))\n",
    "        query_image = img.resize((384, 192))\n",
    "        mask = Image.open(io.BytesIO(mask_file_info['content']))\n",
    "        mask = mask.resize((384, 192))\n",
    "        n_results = result_count.value\n",
    "        match_names, match_values = image_search(query_image, mask, n_results)\n",
    "        print(match_values)\n",
    "        outlined_query = outline_image(query_image, outline)\n",
    "        display_results(outlined_query, match_names, match_values)\n",
    "        \n",
    "\n",
    "def on_show_more_button_clicked(b):\n",
    "    path = os.path.join(database_name, 'print', b.filename)\n",
    "    query = Image.open(path)\n",
    "    n_results = result_count.value\n",
    "    match_names, match_values = image_search(query, n_results)\n",
    "    outlined_query = outline_image(query, outline)\n",
    "    display_results(outlined_query, match_names, match_values)\n",
    "    \n",
    "def image_search(query_image, mask, n_results=24):\n",
    "    img = np.array(query_image)\n",
    "    if len(img.shape) == 3:\n",
    "        img = np.mean(img, axis=2)\n",
    "    img = torch.tensor(img/255.0).to(device).unsqueeze(0).unsqueeze(0)\n",
    "    zeros = torch.zeros(img.shape).to(device)\n",
    "    query = torch.cat((img, zeros), dim=1).float()\n",
    "\n",
    "    mask = np.array(mask)\n",
    "    if len(mask.shape) == 3:\n",
    "        mask = np.mean(mask, axis=2) \n",
    "    mask = torch.tensor(mask/255.0).to(device).unsqueeze(0).unsqueeze(0) > 0.5\n",
    "    \n",
    "    net.eval()\n",
    "    with torch.no_grad():\n",
    "        embedded_mask = (F.interpolate(mask.float(), (6, 12), mode='bilinear') > 0)\n",
    "        query_features = net(query, spatial_feat=True)\n",
    "        query_features = net.vectorize((query_features*embedded_mask.to(query_features.device)).to(query.device), spatial=True)\n",
    "        query_dot_dataset = torch.matmul(query_features, database_features.T)\n",
    "        values, indices = torch.sort(query_dot_dataset, dim=1, descending=True)\n",
    "        indices = indices.squeeze().detach().cpu().numpy()\n",
    "        match_names = database_names[indices][:n_results]\n",
    "        values = values.squeeze().detach().cpu().numpy()\n",
    "        match_values = values[:n_results]\n",
    "    return match_names, match_values\n",
    "\n",
    "device = torch.device('cuda:0') if torch.cuda.is_available() else torch.device('cpu')  \n",
    "print(device)\n",
    "    \n",
    "print('==> Loading model')\n",
    "file_id = '1X8b75CWiFl11VSkhOG0XelQlZrXGhC_Y'\n",
    "# https://drive.google.com/file/d/1FFSh0uSUr5c4ebaCkzIELv98wRizSvrM/view?usp=sharing\n",
    "file_id = '1FFSh0uSUr5c4ebaCkzIELv98wRizSvrM'\n",
    "net = load_model('model.pth', file_id)\n",
    "\n",
    "print('==> Loading database')\n",
    "database_name = 'database'\n",
    "file_id = '1v4DYJ1hF4UadsMOa3yachsORPxwpHQpz'\n",
    "download(file_id, database_name + '.zip', download_name='database.zip')\n",
    "if not os.path.exists(database_name):\n",
    "    with zipfile.ZipFile(database_name + '.zip', 'r') as zip_ref:\n",
    "        zip_ref.extractall()\n",
    "database_feat_name = os.path.join(database_name, 'features.pth')\n",
    "database_features = torch.load(database_feat_name, map_location=device).to(device)\n",
    "database_names = os.path.join(database_name, 'names.npy')\n",
    "database_names = np.load(database_names)\n",
    "\n",
    "# download outline\n",
    "outline_name = 'outline.png'\n",
    "file_id = '1_34Waq_gd7o2KzOjVgUMtVS_3B-KNmSb'\n",
    "download(file_id, outline_name)\n",
    "outline = Image.open(outline_name)\n",
    "\n",
    "clear_output()\n",
    "search_button = widgets.Button(description=\"Search\")\n",
    "# output = widgets.Output()\n",
    "print_uploader = widgets.FileUpload(multiple=False, description='Upload Print')\n",
    "mask_uploader = widgets.FileUpload(multiple=False, description='Upload Mask')\n",
    "result_count = widgets.IntText(\n",
    "    value=10,\n",
    "    description='Count',\n",
    "    disabled=False\n",
    ")\n",
    "search_widget = widgets.HBox([result_count, print_uploader, mask_uploader, search_button], layout=widgets.Layout(justify_content='center'))\n",
    "display(search_widget)\n",
    "search_button.on_click(on_button_clicked)\n",
    "\n",
    "# get metadata\n",
    "with open(os.path.join(database_name, 'metadata.csv'), newline='') as csvfile:\n",
    "    reader = csv.DictReader(csvfile)\n",
    "    metadata = {}\n",
    "    metadata_headers = ['shoeid', 'gender', 'brand', 'product', 'category', 'prints']\n",
    "    for row in reader:\n",
    "        shoeid, gender, brand, product, category, prints = [row[header] for header in metadata_headers]\n",
    "        metadata[int(shoeid)] = {metadata_headers[i+1]: md for i, md in enumerate([gender, brand, product, category, prints])}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
